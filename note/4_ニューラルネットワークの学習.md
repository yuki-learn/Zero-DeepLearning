


## ニューラルネットワークの学習

* パラメータをデータから学習することができるのが、ニューラルネットワークの特徴(end-to-end machine learning) 最初から最後まで
* ニューラルネットワークは与えられたデータをひたすら学習して、与えられた問題パターンを発見しようとする。

## 損失関数
ニューラルネットワークの学習の目的は、**損失関数の値**が一番小さくなるような最適な重みパラメータの探索である。

なぜ、「認識精度」を指標にせず「損失関数の値」を指標にするのか。
-> 認識精度を指標にすると、パラメータの微分がほとんどの場所で0になってしまうから。

「パラメータの微分がほとんどの場所で0になる」というのはどういうことか。

例えば、認識精度を32%の状態だったとして、パラメータを少し改善しただけでは32.0123%のような連続的な変化ではなく、33%、34%のような不連続な値になってしまう。パラメータの微小な変化にはほとんど反応を示さない。

しかし、損失関数を指標にすれば、パラメータの少しの変化も検知し、損失関数の値も変化してくれる。

### 2乗和誤差
最も有名な損失関数。以下数式。
* yk: ニューラルネットワークの出力
* tk: 教師データ

$$
E = \dfrac{1}{2} \sum (yk - tk)^2
$$

#### 実装と解説
[2乗和誤差を使用した損失関数の算出 実装](/src/4/4-1.py)

### 交差エントロピー誤差
* log: 底がeの自然対数
* tk: 正解ラベル
* yk: ニューラルネットワークの出力

tkは正解ラベルとなるインデックスのときだけ1になり、ほかは0となるので、式は実質的に正解ラベルの出力だけしか計算しない。

$$
E = - \sum_k t_k \log y_k
$$

#### 実装
[交差エントロピー誤差を使用した損失関数の算出 実装](/src/4/4-２.py)

### ミニバッチ学習
推論のときと同じように訓練データの損失関数を求める際も、バッチ処理が使えるが、<br>
すべての訓練データを対象にして損失関数の和を求めるには非常に時間がかかる。

そこで、データの一部を選び出してその計算結果を全体の「近似」として利用する。(ミニバッチ)

